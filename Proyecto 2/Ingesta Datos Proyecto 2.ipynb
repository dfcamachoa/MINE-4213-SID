{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c68e46a-b99d-400f-83fb-ea08e09fc53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install zstandard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13ff6e6f-2537-42d3-8eae-b5464ad7a2bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Leyendo los datos desde Crowdsourced Bathymetry in the NOAA Open Data Dissemination (NODD) Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bd657b-c4d8-44eb-831c-c6a939831611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import pandas as pd\n",
    "\n",
    "# Define schema for better performance\n",
    "bathymetry_schema = StructType([\n",
    "    StructField(\"UNIQUE_ID\", StringType(), True),\n",
    "    StructField(\"FILE_UUID\", StringType(), True),\n",
    "    StructField(\"LON\", DoubleType(), True),\n",
    "    StructField(\"LAT\", DoubleType(), True),\n",
    "    StructField(\"DEPTH\", DoubleType(), True),\n",
    "    StructField(\"TIME\", StringType(), True),  # Will convert to timestamp later\n",
    "    StructField(\"PLATFORM_NAME\", StringType(), True),\n",
    "    StructField(\"PROVIDER\", StringType(), True)\n",
    "])\n",
    "\n",
    "def process_bathymetry_efficient(year, month, day):\n",
    "    \"\"\"\n",
    "    Efficiently process bathymetry data\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    bucket = 'noaa-dcdb-bathymetry-pds'\n",
    "    prefix = f'csb/csv/{year}/{month:02d}/{day:02d}/'\n",
    "    \n",
    "    # Get file list\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    \n",
    "    if 'Contents' not in response:\n",
    "        return None\n",
    "    \n",
    "    # Process first file to get actual schema\n",
    "    first_key = response['Contents'][0]['Key']\n",
    "    obj = s3.get_object(Bucket=bucket, Key=first_key)\n",
    "    sample_df = pd.read_csv(obj['Body'], nrows=10)\n",
    "    \n",
    "    print(f\"Columns found: {list(sample_df.columns)}\")\n",
    "    \n",
    "    # Read all files\n",
    "    all_dfs = []\n",
    "    for file_info in response['Contents'][:10]:  # Limit for demo\n",
    "        if file_info['Key'].endswith('.csv'):\n",
    "            obj = s3.get_object(Bucket=bucket, Key=file_info['Key'])\n",
    "            df = pd.read_csv(obj['Body'])\n",
    "            all_dfs.append(df)\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        spark_df = spark.createDataFrame(combined)\n",
    "        \n",
    "        # Convert TIME to timestamp if it exists\n",
    "        if 'TIME' in spark_df.columns:\n",
    "            from pyspark.sql.functions import to_timestamp\n",
    "            spark_df = spark_df.withColumn(\n",
    "                \"TIME\", \n",
    "                to_timestamp(col(\"TIME\"))\n",
    "            )\n",
    "        \n",
    "        return spark_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Process and save to Delta table\n",
    "df = process_bathymetry_efficient(2019, 6, 26)\n",
    "\n",
    "if df:\n",
    "    # Save as Delta table for efficient querying\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(\"bathymetry_data\")\n",
    "    \n",
    "    # Query the saved data\n",
    "    result = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            PLATFORM_NAME,\n",
    "            COUNT(*) as num_readings,\n",
    "            ROUND(AVG(DEPTH), 2) as avg_depth,\n",
    "            ROUND(MIN(LAT), 4) as min_lat,\n",
    "            ROUND(MAX(LAT), 4) as max_lat,\n",
    "            ROUND(MIN(LON), 4) as min_lon,\n",
    "            ROUND(MAX(LON), 4) as max_lon\n",
    "        FROM bathymetry_data\n",
    "        GROUP BY PLATFORM_NAME\n",
    "        ORDER BY num_readings DESC\n",
    "        LIMIT 20\n",
    "    \"\"\")\n",
    "    \n",
    "    result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0ff55c2-d86e-4745-9e8e-032dc02b98ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Leyendo los datos de Marine Cadastre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae828b69-6dca-4d0c-9e04-965026321e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List contents of the marine-cadastre directory\n",
    "display(dbutils.fs.ls(\"/Volumes/proyecto/default/raw_data/marine-cadastre/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8acad35-74f4-458c-b38a-09600f364c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recursively explore the directory structure\n",
    "def explore_directory(path, max_depth=3, current_depth=0):\n",
    "    \"\"\"Recursively explore directory structure\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        items = dbutils.fs.ls(path)\n",
    "        for item in items:\n",
    "            indent = \"  \" * current_depth\n",
    "            if item.isDir():\n",
    "                print(f\"{indent}üìÅ {item.name}\")\n",
    "                explore_directory(item.path, max_depth, current_depth + 1)\n",
    "            else:\n",
    "                size_mb = item.size / (1024 * 1024)\n",
    "                print(f\"{indent}üìÑ {item.name} ({size_mb:.2f} MB)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring {path}: {e}\")\n",
    "\n",
    "# Explore the marine cadastre directory\n",
    "explore_directory(\"/Volumes/proyecto/default/raw_data/marine-cadastre/\", max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c4ebadc-0c62-4b38-84b2-a421a425a873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import pandas as pd\n",
    "import io\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def read_zst_csv_to_spark(file_path):\n",
    "    \"\"\"\n",
    "    Read a .zst compressed CSV file into a Spark DataFrame\n",
    "    \"\"\"\n",
    "    # Read the compressed file\n",
    "    with open(file_path.replace(\"dbfs:\", \"/dbfs\"), 'rb') as f:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        decompressed = dctx.decompress(f.read())\n",
    "        \n",
    "        # Convert bytes to string and read as CSV\n",
    "        csv_string = decompressed.decode('utf-8')\n",
    "        df_pandas = pd.read_csv(io.StringIO(csv_string))\n",
    "        \n",
    "        # Convert to Spark DataFrame\n",
    "        return spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Read a single file first to understand the structure\n",
    "sample_file = \"/Volumes/proyecto/default/raw_data/marine-cadastre/ais-2025-01-01.csv.zst\"\n",
    "sample_df = read_zst_csv_to_spark(sample_file)\n",
    "\n",
    "print(\"AIS Data Schema:\")\n",
    "sample_df.printSchema()\n",
    "print(f\"\\nNumber of records: {sample_df.count()}\")\n",
    "sample_df.show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta Datos Proyecto 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
